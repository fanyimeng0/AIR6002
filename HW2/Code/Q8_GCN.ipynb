{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Convolutional Networks (GCNs) are similar to convolutions in images in the sense that the \"filter\" parameters are typically shared over all locations in the graph. At the same time, GCNs rely on message passing methods, which means that vertices exchange information with the neighbors, and send \"messages\" to each other. Before looking at the math, we can try to visually understand how GCNs work. \n",
    "\n",
    "- The first step is that each node creates a feature vector that represents the message it wants to send to all its neighbors. \n",
    "\n",
    "- In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. Below we have visualized the two steps for our example graph. \n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"graph_message_passing.svg\" width=\"700px\"></center>\n",
    "\n",
    "If we want to formulate that in more mathematical terms, we need to first decide how to combine all the messages a node receives. As the number of messages vary across nodes, we need an operation that works for any number. Hence, the usual way to go is to sum or take the mean. Given the previous features of nodes $H^{(l)}$, the GCN layer is defined as follows:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
    "\n",
    "$W^{(l)}$ is the weight parameters with which we transform the input features into messages ($H^{(l)}W^{(l)}$). To the adjacency matrix $A$ we add the identity matrix so that each node sends its own message also to itself: $\\hat{A}=A+I$. Finally, to take the average instead of summing, we calculate the matrix $\\hat{D}$ which is a diagonal matrix with $D_{ii}$ denoting the number of neighbors node $i$ has. $\\sigma$ represents an arbitrary activation function, and not necessarily the sigmoid (usually a ReLU-based activation function is used in GNNs). \n",
    "\n",
    "When implementing the GCN layer in PyTorch, we can take advantage of the flexible operations on tensors. Instead of defining a matrix $\\hat{D}$, we can simply divide the summed messages by the number of neighbors afterward. Additionally, we replace the weight matrix with a linear layer, which additionally allows us to add a bias. Written as a PyTorch module, the GCN layer is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        \n",
    "        #===============================================================\n",
    "        # TODO: Calculate the number of neighbors for each node\n",
    "        degree_matrix = torch.sum(adj_matrix, dim=-1)\n",
    "        degree_matrix_inv_sqrt = torch.diag_embed(1.0 / torch.sqrt(degree_matrix + 1e-6))\n",
    "        # TODO: Apply a linear projection to the node features\n",
    "        norm_adj_matrix = torch.matmul(degree_matrix_inv_sqrt, adj_matrix)\n",
    "        norm_adj_matrix = torch.matmul(norm_adj_matrix, degree_matrix_inv_sqrt)\n",
    "        # TODO: Aggregate neighbor features for each node\n",
    "        aggregated_feats = torch.matmul(norm_adj_matrix, node_feats)\n",
    "        # TODO: Normalize aggregated features by the number of neighbors\n",
    "        node_feats = self.projection(aggregated_feats)\n",
    "\n",
    "        # 第2步：对邻接矩阵进行标准化\n",
    "   \n",
    "\n",
    "        # 第3步：聚合邻居特征\n",
    "        \n",
    "\n",
    "        # 第4步：对节点特征应用线性投影\n",
    "\n",
    "\n",
    "        #===============================================================\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further understand the GCN layer, we can apply it to our example graph above. First, let's specify some node features and the adjacency matrix with added self-connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "\n",
    "# TODO: Define the adjacency matrix in the graph above with self-connections\n",
    "adj_matrix =  torch.ones(1, 4, 4) \n",
    "\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply a GCN layer to it. For simplicity, we initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[3.0000, 4.0000],\n",
      "         [3.0000, 4.0000],\n",
      "         [3.0000, 4.0000],\n",
      "         [3.0000, 4.0000]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
