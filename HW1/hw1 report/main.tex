%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font
\usepackage{amsmath}
\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Homework \#1} % Assignment title

\author{Fanyi Meng (223015127)} % Student name

\date{January 31st, 2024} % Due date

\institute{The Chinese University of Hongkong, Shenzhen \\ Computer and Information Engineering} % Institute or school name

\class{Advanced Machine Learning (AIR 6002)} % Course or class name

\professor{Prof Tongxin Li} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------

\section*{Question 1: Basics}
\begin{problem}

	
	\begin{enumerate}[(\itshape a\normalfont)]
		\itemsep0.3em
		\parskip0.3em
		\item What is a hypothesis set?
		\item What is the hypothesis set of a linear model?
		\item What is overfitting?
		\item What are two ways to prevent overfitting?
		\item What are training data and test data, and how are they used differently? Why should you never change your model based on information from test data?
		\item What are the two assumptions we make about how our dataset is sampled?
		\item Consider the machine learning problem of deciding whether or not an email is spam. What could \(X\), the input space, be? What could \(Y\), the output space, be?
		\item What is the \(k\)-fold cross-validation procedure?
	\end{enumerate}
	

\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\itemsep0.3em
	\parskip0.3em
	\item Hypothesis set contains all possible models on a specific dataset.
	\item Hypothesis set of a linear model means the model can be represented by linear model with its parametes. It can be writen as: $y = \sum_{i = 1}^{n}\theta_i x_i+\theta_0  $
	\item Overfitting means the model performs well in the train model but not as well in test model, because the model learns some irrelevant knowledge or random noise. 
	\item Adding Regularization and using a lower learning rate can prevent overfitting.
	\item Training data is used to train the model and test data is used to test the model's performance. Changing model based on information from test data will improve the performance incorrectly 
	because the model have "seen" the test data, which would cause the model can't have a matched performance in read-world data.
	\item Indepedent and Identically Distributed.
	\item \textbf{Input}:Email Address, Email Content, the network information(IP/MAC Address) \textbf{Output}:spam or not
	\item Firstly the dataset is devided into k subsets, and then the model will be trained for k times, in each time one subset will be used to test the model and the other subsets will be used to trian. Finally the final model performance will be calculated as the average of the k models.
\end{enumerate}

%------------------------------------------------
%----------------------------------------------------------------------------------------

\section*{Question 2: Bias-Variance Tradeoff}

\begin{problem}
	\begin{enumerate}[(\itshape a\normalfont)]
	\itemsep0.3em
	\parskip0.3em
	\item Derive the bias-variance decomposition for the squared error loss function. That is, show that for a model $f_S$ trained on a dataset $S$ to predict a target $y(x)$ for each $x$,

	$$
	\begin{aligned}\mathbb{E}_{S}\left[E_{\mathrm{out}}\:(f_{S})\right]=\mathbb{E}_{x}[\mathrm{Bias}(x)+\mathrm{Var}(x)]\end{aligned}
	$$
	$\mathrm{given~the~following~definitions: }$

$$
\begin{aligned}
F(x)& =\mathbb{E}_{S}\left[f_{S}(x)\right]  \\
E_{\mathrm{out}}\:(f_{S})& =\mathbb{E}_x\left[\left(f_S(x)-y(x)\right)^2\right]  \\
\operatorname{Bias}(x)& =(F(x)-y(x))^{2}  \\
\operatorname{Var}(x)& =\mathbb{E}_{S}\left[(f_{S}(x)-F(x))^{2}\right] 
\end{aligned}
$$

	\item For each \( N \in \{ 20, 25, 30, 35, \cdots, 100\} \):
	\begin{enumerate}[i.]
	  \item Perform 5-fold cross-validation on the first \( N \) points in the dataset (setting aside the other points), computing both the training and validation error for each fold.
		\begin{itemize}
		  \item Use the mean squared error loss as the error function.
		  \item Use NumPy's \texttt{polyfit} method to perform the degree-\( d \) polynomial regression, and NumPy's \texttt{polyval} method to help compute the errors. (Refer to example code and NumPy documentation for details.)
		  \item When partitioning your data into folds, divide the data into \( K \) contiguous blocks (though in practice, you should randomize your partitions, for the purpose of this exercise, use contiguous blocks).
		\end{itemize}
	  \item Compute the average of the training and validation errors from the 5 folds.
	  \item Create a learning curve by plotting both the average training and validation error as functions of \( N \).
	\end{enumerate}
  
	

\end{enumerate}
\end{problem}

%------------------------------------------------
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
\item 
\begin{equation*}
	\begin{aligned}
		E_{out}(f_S) &= \mathbb{E}_x[(f_S(x)-y(x))^2] \\
		 &= \mathbb{E}_x[f_S(x)^2-2f_S(x)y(x)+y(x)^2]
	\end{aligned}
\end{equation*}
By rewriting $f_S(x)$ as $F(x)+(f_S(x)-F(x))$,$E_{out}(f_S)$ can be represented as 
\begin{equation*}
	\begin{aligned}
	E_{out}(f_S)&=\mathbb{E}_x[(F(x)+(f_S(x)-F(x)))^2-2(F(x)+(f_S(x)-F(x)))y(x)+y(x)^2] \\
	            &=\mathbb{E}_x[(F(x)-y(x))^2+(f_S(x)-F(x))^2+2(f_S(x)-F(x))(F(x)-y(x))] \\
	\end{aligned}
\end{equation*}
As $F(x)=\mathbb{E}_{S}\left[f_{S}(x)\right]$, $\mathbb{E}_{S}(f_S(x)-F(x))(F(x)-y(x))$ equals to zero. So:
\begin{equation*}
	\mathbb{E}_{S}\left[E_{\mathrm{out}}\:(f_{S})\right]=\mathbb{E}_{X}[\underbrace{(F(x)-y(x))^{2}}_{\mathrm{Bias}(x)}+\underbrace{(f_{S}(x)-F(x))^2}_{\mathrm{Var}(x)}]
\end{equation*}
\item haha
\end{enumerate}
%----------------------------------------------------------------------------------------
\section*{Question 3}

\begin{problem}
	Find the closed-form solutions of the following optimization problems $(\mathbf{W}\in\mathbb{R}^{K\times D},N\gg D>$ $K)\colon$
	\medskip
    \begin{enumerate}[(\itshape a\normalfont)]
		\itemsep0.3em
		\parskip0.3em
		
	\item $\underset{W, b}{\min}\sum _{i= 1}^N\|\mathbf{y} _i- \mathbf{W} \mathbf{x} _i- \mathbf{b} \|^2$  
	\item $\underset{W, b}{\min}\sum _{i= 1}^N\|\mathbf{y} _i- \mathbf{W} \mathbf{x} _i- \mathbf{b} \|^2+ \frac \lambda2\|\mathbf{W} \|_F^2$
\end{enumerate}
\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\item Denote $\overline{\mathbf{W}} = [b , w], \overline{\mathbf{x} _i} = [1 , x_i ]^\top$, then the objective function can be rewriten as:

\begin{equation*}
    \sum _{i= 1}^N\|\mathbf{y} _i- \mathbf{W} \mathbf{x} _i- \mathbf{b} \|^2 = \|\mathbf{Y} - \overline{\mathbf{W}}  \overline{\mathbf{X}} \|^2_F
\end{equation*}
Then, the gradient on $\mathbf{W}$ is:
\begin{equation*}
    \frac{\nabla\|\mathbf{Y} - \overline{\mathbf{W}}  \overline{\mathbf{X}} \|^2_F}{\nabla\overline{\mathbf{W}}}=2\overline{\mathbf{W}}\overline{\mathbf{X}} \overline{\mathbf{X}}^{\top}  - 2\mathbf{Y}\overline{\mathbf{X}}^{\top}
\end{equation*}
Let the gradient equals to zero, we can get the optimal $\overline{\mathbf{W}} = \mathbf{Y}\overline{\mathbf{X}}^{\top}\left(\overline{\mathbf{X}}\overline{\mathbf{X}}^{\top} \right)^{-1} $

\item Similarly,we can get the result $\overline{\mathbf{W}} = \mathbf{Y}\overline{\mathbf{X}}^{\top}\left(\overline{\mathbf{X}}\overline{\mathbf{X}}^{\top} +\lambda\mathbf{I} \right)^{-1}$
\end{enumerate}
%------------------------------------------------

\section*{Question 4}

\begin{problem}
 Consider the following problem

$$
\underset{\mathbf{W}}{\min}\:\frac12\|\mathbf{W}\Phi-\mathbf{Y}\|_{F}^{2}+\frac\lambda2\|\mathbf{W}\|_{F}^{2}
$$

where $\|\cdot \|_F$ denotes the Frobenius norm; $Y \in \mathbb{R} ^K\times N, \Phi= [ \phi( \mathbf{x} _1) , \phi( \mathbf{x} _2) , \ldots , \phi( \mathbf{x} _N) ] , \mathbf{x} _i\in \mathbb{R} ^D, $ $i=1,2,\ldots,N$ and $\phi$ is the feature map induced by a kernel function $k(\cdot,\cdot)$. Prove that for any x$\in \mathbb{R} ^D, $ we can make prediction as

$$
\mathbf{y}=\mathbf{W}\phi(\mathbf{x})=\mathbf{Y}\left(\mathbf{K}+\lambda\mathbf{I}\right)^{-1}\mathbf{k}(\mathbf{x}),
$$

where $\mathbf{K} = \Phi^\top\Phi$ and $\mathbf{k}(\mathbf{x}) = [k(\mathbf{x}_1, \mathbf{x}), k(\mathbf{x}_2, \mathbf{x}), \ldots, k(\mathbf{x}_N, \mathbf{x})]^\top$.
\end{problem}
\section*{Answer}
By calculating the gradient, we get:
\begin{equation*}
	W \Phi^{\top}\Phi - Y\Phi + \lambda W = 0
\end{equation*}
\begin{equation*}
	W = Y \Phi (\Phi^{\top}\Phi + \lambda I)^{-1}
\end{equation*}
Considering $\mathbf{K} = \Phi^\top\Phi$ and $\mathbf{k}(\mathbf{x}) = \Phi\phi(x)^\top$.
We can predict $y$ by:
\begin{equation*}
	\begin{aligned}
		y &= W\phi(x) \\
		  &= [Y \Phi (\Phi^{\top}\Phi + \lambda I)^{-1}]\phi(x) \\
		  &= Y(K + \lambda I)^{-1}k(x)
	\end{aligned}
\end{equation*}
\section*{Question 5}

\begin{problem}
	Compute the space and time complexities (in the form of big O, consider only the training
stage) of the following algorithms:
\begin{enumerate}[(\itshape a\normalfont)]
\item Ridge regression (Question 2(b)) with the closed-form solution 
\item $N$ data points of $D\mathrm{- dimension, ~choose~}d$ principal components$) $
\item Neural network with architecture $D-H_1-H_2-K$ on a mini-batch of size $B$ (consider only
the forward process and neglect the computational costs of activation functions) 

[Hint: the time complexity of $A\in\mathbb{R}^{m\times n}\times B\in\mathbb{R}^{n\times l}$ is $O(mnl);$ the time complexities of
 eigenvalue decomposition and inverse of an $n\times n$ matrix are both $O(n^3).]$
\end{enumerate}
\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\item \textbf{Time complexity:} ${\mathcal O}(ND^{2}+D^{3})$ \textbf{Space complexity:} ${\mathcal O}(ND+D^{2})$
	\item \textbf{Time complexity:} ${\mathcal O}(ND^{2}+D^{3})$ \textbf{Space complexity:} ${\mathcal O}(ND+D^{2})$
	\item \textbf{Time complexity:} ${\mathcal O}(BDH_1+BH_1H_2+BH_2K)$ \textbf{Space complexity:} ${\mathcal O}(BD+BH_1+BH_2+BK+DH_1+H_1H_2+H_2K)$
\end{enumerate}


\section*{Question 6}

\begin{problem}
	Prove the convergence of the generic gradient boosting algorithm (AnyBoost). Specifically, suppose in the algorithm of AnyBoost (page 14 of Lecture 02), the gradient of the objective function $\mathcal{L}$ is L-Lipschitz continuous, i.e., there exists $L>0$ such that

$$
\|\nabla\mathcal{L}(H)-\nabla\mathcal{L}(H^{\prime})\|\leq L\|H-H^{\prime}\|
$$

holds for any $H$ and $H^{\prime}$. Suppose in the algorithm, $\alpha$ is computed as

$$
\alpha_{t+1}=-\frac{\langle\nabla\mathcal{L}(H_{t}),h_{t+1}\rangle}{L\|h_{t+1}\|^{2}}.
$$

Then the ensemble model is updated as $H_{t+1}=H_t+\alpha_{t+1}h_{t+1}.$ Prove that the algorithm either terminates at round $T$ with $\langle\nabla\mathcal{L}(H_t),h_{t+1}\rangle$ or $\mathcal{L}(H_t)$ converges to a finite value, in which case

$$
\operatorname*{lim}_{t\to\infty}\langle\nabla\mathcal{L}(H_{t}),h_{t+1}\rangle=0.
$$

* [Hint: Using the following result: Suppose $\mathcal{L} : \mathcal{H} \to \mathbb{R} $ and $\|\nabla\mathcal{L} ( F) - \nabla\mathcal{L} ( G) \|\leq L\|F- G\|$ holds for any $F$ and $G$ in $\mathcal{H} $, then $\mathcal{L}(F+wG)-\mathcal{L}(F)\leq w\langle\nabla\mathcal{L}(F),G\rangle+\frac{Lw^2}2\|G\|^2$ holds for any $w>0.]$

\end{problem}
\section*{Answer}
By using the Hint, 
\begin{equation*}
	\begin{aligned}
		\mathcal{L}(H_{t+1})-\mathcal{L}(H_{t})& =\mathcal{L}(H_{t}+\alpha_{t+1}h_{t+1})-\mathcal{L}(H_{t})l  \\
		&\leq\alpha_{t+1}\left<\nabla \mathcal{L}(H_{t}),h_{t+1}\right>+\frac{L\alpha _{t+1}^{2}\|h_{t+1}\|^{2}}{2} \\
		& = -\frac{\langle\nabla \mathcal{L}(H_{t}),h_{t+1}\rangle^{2}}{2L\|h_{t+1}\|^{2}}.
	\end{aligned}
\end{equation*}
Considering when $h_{T+1}=0$, the algorithm will terminate at time $T$, otherwise, which means $h_{t+1}\neq 0$ for all $t$
 as $\mathcal{L}(H_{t+1})-\mathcal{L}(H_{t}) \rightarrow 0$,  $\operatorname*{lim}_{t\to\infty}\langle\nabla\mathcal{L}(H_{t}),h_{t+1}\rangle$ will also converge to 0.
\section*{Question 7:SGD}

\begin{problem}
	Linear regression learns a model of the form:
	$$f\left(x_{1},x_{2},\cdots,x_{d}\right)=\left(\sum_{i=1}^{d}w_{i}x_{i}\right)+b$$
	\begin{enumerate}[(\itshape a\normalfont)]
	\item We can make our algebra and coding simpler by writing $f\left(x_1,x_2,\cdots,x_d\right)=\mathbf{w}^T\mathbf{x}$ for vectors w and x. But at first glance, this formulation seems to be missing the bias term $b$ from the equation above. How should we define x and w such that the model includes the bias term?
	
\end{enumerate}
\medskip
Linear regression learns a model by minimizing the squared loss function $L$, which is the sum across all training data $\{(\mathbf{x}_1,y_1),\cdots,(\mathbf{x}_N,y_N)\}$ of the squared difference between actual and predicted output values:
$$L(f)=\sum_{i=1}^N\left(y_i-\mathbf{w}^T\mathbf{x}_i\right)^2$$
\begin{enumerate}[(\itshape b\normalfont)]
	\item  SGD uses the gradient of the loss function to make incremental adjustments to the weight vector $w$. Derive the gradient of the squared loss function with respect to $w$ for linear regression.
\end{enumerate}
(c)-(f) Coding Part.
\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\item By defineing $\mathbf{x} = (x_1,x_2,\cdots,x_d, 1)$ and $\mathbf{w} = (w_1,w_2,\cdots,x_n, b)$
	\item $ \nabla \mathbf{w}= -2 \cdot (\mathbf{y_i} - \mathbf{x_i} \cdot \mathbf{w}) \mathbf{x_i}$

\end{enumerate}
\section*{Question 8}

\begin{problem}
True or False? If False, then explain shortly.
\medskip
\begin{enumerate}[(\itshape a\normalfont)]
\item The inequality $G(\mathcal{F},n)\leq n^2$ holds for any model class $\mathcal{F}.$
\item The VC dimension of an axis-aligned rectangle in a 2D space is 4.
\item The VC dimension of a circle in a 2D space is 4.
\item The VC dimension of 1-nearest neighbor classifier in $d$-dimensional space is $d+1.$
\item Let $d$ be the VC dimension of $\mathcal{F}$. Then the inequality $G(\mathcal{F},n)\leq\left(\frac{en}d\right)^d$ always holds.
\end{enumerate}
\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\item \textbf{False}.  When n=1, $G(\mathcal{F},n)$ can take the value 2 for $f_1$ predict 1 and $f_2$ predict -1, but $n^2$ equals to 1.
	\item \textbf{True}. 4 points can be scattered by 4 sides of rectangle.
	\item \textbf{False}. VC dimension of a circle is 3.
	\item \textbf{False}. VC dimension of 1-nearest neighbor classifier is infinity.
	\item \textbf{False}. For $n=1$,$G(\mathcal{F},n)$ is 2 but $\left(\frac{en}d\right)^d<1$ for $d>e$
\end{enumerate}

\section*{Question 9}

\begin{problem}
In LASSO, the model class is defined as $\mathcal{F}=\{\mathbf{x}\mapsto\langle\mathbf{w},\mathbf{x}\rangle:\|\mathbf{w}\|_1\leq\alpha\}.$ Suppose $\mathbf{x}\in\mathbb{R}^d$, $y\in \{ - 1, + 1\} , $ the training data are $S= \{ ( \mathbf{x} _i, y_i) \} _i= 1^n, \text{ and max}_{1\leq i\leq n}\|\mathbf{x} _i\|_\infty\leq \beta, $ where $\|\cdot \|_\infty$ denotes the largest absolute element of a vector.
\medskip
\begin{enumerate}[(\itshape a\normalfont)]
\item Find an upper bound of the empirical Rademacher complexity, where $\sigma_i$ are the Rademacher variables. 
\item Suppose the loss function is the absolute loss. Use the inequality (highlighted in blue) on page
$30$ and Lemma 5 on page 35 (i.e., $(\mathrm{i.e.},\mathcal{R}(\ell\circ\mathcal{F})\leq\eta\mathcal{R}(\mathcal{F}))$) of Lecture 03 to derive a generalization error bound for LASSO.


  * Hint: For question (a), please use the inequality $\langle\mathbf{a},\mathbf{b}\rangle\leq\|\mathbf{a}\|_1\|\mathbf{b}\|_\infty$ and the following lemma:


  \textbf{Lemma 1.} Let \( A \subseteq \mathbb{R}^n \) be a finite set of points with \( r = \max_{\mathbf{x} \in A} \|\mathbf{x}\|_2 \) and denote \( \mathbf{x} = (x_1, x_2, \ldots, x_n) \). Then
  $$\mathbb{E}_{\sigma}\left[\max_{\mathbf{x}\in A}\sum_{i=1}^nx_i\sigma_i\right]\leq r\sqrt{2\log|A|}$$
$where\:|A|\:denotes\:the\:cardinality\:of\:set\:A\:and\:\sigma_i\:are\:the\:Rademacher\:variables.$
\end{enumerate}
\end{problem}
\section*{Answer}
\begin{enumerate}[(\itshape a\normalfont)]
	\item The empirical Rademacher complexity can be written as :
	\begin{equation*}
		\begin{aligned}
			\mathcal{R}(\mathcal{F}) & ={\mathbb{E}_{\sigma}}\left[\sup_{f\in\mathcal{F}}\frac1n\sum_{i=1}^n\sigma_i f(x_i)\right] \\
									 & ={\mathbb{E}_{\sigma}}\left[\sup_{\substack{w_i\in\mathcal{W}\\x_i\in\mathcal{X}}}\frac1n\sum_{i=1}^n\sigma_i \langle w_i, x_i \rangle \right]
		\end{aligned}	
	\end{equation*}
	By using the inequality of $\langle\mathbf{a},\mathbf{b}\rangle\leq\|\mathbf{a}\|_1\|\mathbf{b}\|_\infty$
	\begin{equation*}
		\mathcal{R}(\mathcal{F})  \leq {\mathbb{E}_{\sigma}}\left[\sup_{\substack{w_i\in\mathcal{W}\\x_i\in\mathcal{X}}}\frac1n\sum_{i=1}^n\sigma_i \|\mathbf{w_i}\|_1 \|\mathbf{x} _i\|_\infty\right]								 
	\end{equation*}
	Then by using \textbf{Lemma 1},  $\|\mathbf{w}\|_1\leq\alpha\ max_{1\leq i\leq n}\|\mathbf{x} _i\|_\infty\leq \beta $ and $\|\mathbf{w x}\|_2 \leq \|\mathbf{w}\|_2\|\mathbf{x}\|_2$ we can get the upper bound:
	\begin{equation*}
		\begin{aligned}
		\mathcal{R}(\mathcal{F}) & \leq \frac1n{\mathbb{E}_{\sigma}}\left[\sup_{\substack{w_i\in\mathcal{W}\\x_i\in\mathcal{X}}}\sum_{i=1}^n\sigma_i \|\mathbf{w_i}\|_1 \|\mathbf{x} _i\|_\infty\right]	\\	
								 &\leq \frac1n{\mathbb{E}_{\sigma}}\left[\sup_{\substack{w_i\in\mathcal{W}\\x_i\in\mathcal{X}}}\sum_{i=1}^n\sigma_i \alpha \beta \right]	\\
								 & \leq \frac1n\alpha \beta \sqrt{2\log n}
	\end{aligned}
	\end{equation*}
	\item By using the f
\end{enumerate}

%------------------------------------------------




%----------------------------------------------------------------------------------------

\end{document}
